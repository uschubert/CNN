{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras.utils\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "#import math\n",
    "#import time\n",
    "#import h5py\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input2=[layers.Input(shape=(len(X_train[i][0]),)) for i in range(1,len(X_train))]\n",
    "    input1 = layers.Input(shape = (grid, grid,1))\n",
    "    x = layers.Conv2D(32, (5, 5), activation='relu',padding='same')(input1)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu',padding='same')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu',padding='same')(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x1 = layers.Flatten()(x)\n",
    "    x = layers.concatenate(inputs = [x1]+input2,axis=-1)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    output = layers.Dense(2, activation='softmax')(x)\n",
    "    model = models.Model(inputs=[input1] +input2, outputs=output)\n",
    "    opt=keras.optimizers.Adam(lr=0.0005,beta_1=0.9, beta_2=0.9, amsgrad=False)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_XY(features,label,dic):\n",
    "    X_test=[dic[key] for key in features]\n",
    "    Y_test=[dic[key] for key in label]\n",
    "    X_test[0]=X_test[0].reshape(n_test, grid, grid, 1)\n",
    "    dim=[1 if ele[0].shape==() else ele[0].shape[0] for ele in X_test]\n",
    "    for i in range(1,len(X_test)):\n",
    "        X_test[i]=X_test[i].reshape(len(X_test[i]),dim[i])\n",
    "    return X_test,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data:\n",
    "Using Train and Test set produced by preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 16\n",
    "data_train= np.load('data/ShowJet_full_train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893412\n",
      "99270\n"
     ]
    }
   ],
   "source": [
    "n_train=len(data_train['jetPt'])\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetImages',\n",
       " 'jetPt',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'tau21',\n",
       " 'chMult',\n",
       " 'neutMult',\n",
       " 'phoMult',\n",
       " 'eleMult',\n",
       " 'muMult',\n",
       " 'jetpull',\n",
       " 'Abs_jetpull']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all=[key for key in data_train.keys()];feat_all.remove('labels');feat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model with all Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features=['jetImages',\n",
    " 'jetPt',\n",
    " 'jetEta',\n",
    " 'jetPhi',\n",
    " 'tau21',\n",
    " 'chMult',\n",
    " 'neutMult',\n",
    " 'phoMult',\n",
    " 'eleMult',\n",
    " 'muMult',\n",
    " 'jetpull']\n",
    "# label into Y\n",
    "label=['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train=build_XY(features,label,data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's prediction $before$ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 9ms/step\n",
      "[[0.52069694 0.47930306]\n",
      " [0.5344897  0.46551028]\n",
      " [0.5261534  0.47384658]\n",
      " [0.52999145 0.47000852]\n",
      " [0.5235912  0.4764088 ]\n",
      " [0.5062274  0.49377266]\n",
      " [0.5210249  0.47897512]\n",
      " [0.5147497  0.4852503 ]\n",
      " [0.5179273  0.48207277]\n",
      " [0.52657217 0.47342786]]\n",
      "[0.6499219536781311, 0.6499219536781311, 1.0]\n"
     ]
    }
   ],
   "source": [
    "X_batch =[ele[:10] for ele in X_train]\n",
    "Y_batch =[ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y =Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train! (warning: if building CNN, computer tends to get loud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/4\n",
      "714729/714729 [==============================] - 1109s 2ms/step - loss: 0.4144 - binary_crossentropy: 0.4144 - acc: 0.8201 - val_loss: 0.4972 - val_binary_crossentropy: 0.4972 - val_acc: 0.7769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49717, saving model to model/CNN_all.h1\n",
      "Epoch 2/4\n",
      "714729/714729 [==============================] - 1083s 2ms/step - loss: 0.3899 - binary_crossentropy: 0.3899 - acc: 0.8363 - val_loss: 0.5700 - val_binary_crossentropy: 0.5700 - val_acc: 0.7404\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49717\n",
      "Epoch 3/4\n",
      "714729/714729 [==============================] - 1094s 2ms/step - loss: 0.3852 - binary_crossentropy: 0.3852 - acc: 0.8393 - val_loss: 0.5001 - val_binary_crossentropy: 0.5001 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49717\n",
      "Epoch 4/4\n",
      "714729/714729 [==============================] - 1265s 2ms/step - loss: 0.3828 - binary_crossentropy: 0.3828 - acc: 0.8408 - val_loss: 0.5065 - val_binary_crossentropy: 0.5065 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49717\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_all.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', \n",
    "                                   verbose=1, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', \n",
    "                                   period=1)    \n",
    "EPOCHS = 4\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history = CNN.fit(\n",
    "  X_train, Y_train,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose = 1,\n",
    "  callbacks=[early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build 10 Models with one Expert and fit with subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 760s 1ms/step - loss: 0.4989 - binary_crossentropy: 0.4989 - acc: 0.7595 - val_loss: 0.6870 - val_binary_crossentropy: 0.6870 - val_acc: 0.6358\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68697, saving model to model/CNN_sub_1.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 640s 896us/step - loss: 0.4674 - binary_crossentropy: 0.4674 - acc: 0.7827 - val_loss: 0.7175 - val_binary_crossentropy: 0.7175 - val_acc: 0.6324\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68697\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 652s 913us/step - loss: 0.4627 - binary_crossentropy: 0.4627 - acc: 0.7861 - val_loss: 0.7016 - val_binary_crossentropy: 0.7016 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68697\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 668s 934us/step - loss: 0.5322 - binary_crossentropy: 0.5322 - acc: 0.7448 - val_loss: 0.8614 - val_binary_crossentropy: 0.8614 - val_acc: 0.5033\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86136, saving model to model/CNN_sub_2.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 676s 946us/step - loss: 0.5078 - binary_crossentropy: 0.5078 - acc: 0.7643 - val_loss: 0.7700 - val_binary_crossentropy: 0.7700 - val_acc: 0.5614\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.86136 to 0.77003, saving model to model/CNN_sub_2.h1\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 671s 939us/step - loss: 0.5034 - binary_crossentropy: 0.5034 - acc: 0.7676 - val_loss: 0.9195 - val_binary_crossentropy: 0.9195 - val_acc: 0.5179\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.77003\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 580s 812us/step - loss: 0.5357 - binary_crossentropy: 0.5357 - acc: 0.7421 - val_loss: 0.8719 - val_binary_crossentropy: 0.8719 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87195, saving model to model/CNN_sub_3.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 610s 853us/step - loss: 0.5073 - binary_crossentropy: 0.5073 - acc: 0.7649 - val_loss: 0.9382 - val_binary_crossentropy: 0.9382 - val_acc: 0.4811\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.87195\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 612s 857us/step - loss: 0.5031 - binary_crossentropy: 0.5031 - acc: 0.7681 - val_loss: 0.8590 - val_binary_crossentropy: 0.8590 - val_acc: 0.5163\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.87195 to 0.85903, saving model to model/CNN_sub_3.h1\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 624s 873us/step - loss: 0.4720 - binary_crossentropy: 0.4720 - acc: 0.7826 - val_loss: 0.6532 - val_binary_crossentropy: 0.6532 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65321, saving model to model/CNN_sub_4.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 613s 857us/step - loss: 0.4639 - binary_crossentropy: 0.4639 - acc: 0.7887 - val_loss: 0.6089 - val_binary_crossentropy: 0.6089 - val_acc: 0.6985\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65321 to 0.60886, saving model to model/CNN_sub_4.h1\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 567s 793us/step - loss: 0.4641 - binary_crossentropy: 0.4641 - acc: 0.7894 - val_loss: 0.6410 - val_binary_crossentropy: 0.6410 - val_acc: 0.6837\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60886\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 550s 769us/step - loss: 0.5308 - binary_crossentropy: 0.5308 - acc: 0.7469 - val_loss: 0.6352 - val_binary_crossentropy: 0.6352 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63517, saving model to model/CNN_sub_5.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 534s 747us/step - loss: 0.4985 - binary_crossentropy: 0.4985 - acc: 0.7711 - val_loss: 0.6738 - val_binary_crossentropy: 0.6738 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.63517\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 545s 762us/step - loss: 0.4876 - binary_crossentropy: 0.4876 - acc: 0.7787 - val_loss: 0.6669 - val_binary_crossentropy: 0.6669 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63517\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 505s 706us/step - loss: 0.5309 - binary_crossentropy: 0.5309 - acc: 0.7466 - val_loss: 0.7026 - val_binary_crossentropy: 0.7026 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70260, saving model to model/CNN_sub_6.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 633s 885us/step - loss: 0.5051 - binary_crossentropy: 0.5051 - acc: 0.7667 - val_loss: 0.7038 - val_binary_crossentropy: 0.7038 - val_acc: 0.6248\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.70260\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 870s 1ms/step - loss: 0.4978 - binary_crossentropy: 0.4978 - acc: 0.7713 - val_loss: 0.7380 - val_binary_crossentropy: 0.7380 - val_acc: 0.5949\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.70260\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 868s 1ms/step - loss: 0.5364 - binary_crossentropy: 0.5364 - acc: 0.7432 - val_loss: 0.8040 - val_binary_crossentropy: 0.8040 - val_acc: 0.5319\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.80395, saving model to model/CNN_sub_7.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 824s 1ms/step - loss: 0.5046 - binary_crossentropy: 0.5046 - acc: 0.7664 - val_loss: 0.8582 - val_binary_crossentropy: 0.8582 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.80395\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 553s 774us/step - loss: 0.4949 - binary_crossentropy: 0.4949 - acc: 0.7729 - val_loss: 0.7514 - val_binary_crossentropy: 0.7514 - val_acc: 0.5931\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80395 to 0.75143, saving model to model/CNN_sub_7.h1\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 505s 707us/step - loss: 0.5331 - binary_crossentropy: 0.5331 - acc: 0.7439 - val_loss: 0.6852 - val_binary_crossentropy: 0.6852 - val_acc: 0.6201\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68525, saving model to model/CNN_sub_8.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 502s 702us/step - loss: 0.5068 - binary_crossentropy: 0.5068 - acc: 0.7646 - val_loss: 0.7433 - val_binary_crossentropy: 0.7433 - val_acc: 0.6028\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68525\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 502s 703us/step - loss: 0.5015 - binary_crossentropy: 0.5015 - acc: 0.7684 - val_loss: 0.7164 - val_binary_crossentropy: 0.7164 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68525\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 504s 705us/step - loss: 0.5271 - binary_crossentropy: 0.5271 - acc: 0.7483 - val_loss: 0.7835 - val_binary_crossentropy: 0.7835 - val_acc: 0.5050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78352, saving model to model/CNN_sub_9.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 501s 701us/step - loss: 0.5045 - binary_crossentropy: 0.5045 - acc: 0.7663 - val_loss: 0.7772 - val_binary_crossentropy: 0.7772 - val_acc: 0.5388\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78352 to 0.77722, saving model to model/CNN_sub_9.h1\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 500s 700us/step - loss: 0.4997 - binary_crossentropy: 0.4997 - acc: 0.7696 - val_loss: 0.7218 - val_binary_crossentropy: 0.7218 - val_acc: 0.6101\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77722 to 0.72176, saving model to model/CNN_sub_9.h1\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/3\n",
      "714729/714729 [==============================] - 509s 712us/step - loss: 0.5337 - binary_crossentropy: 0.5337 - acc: 0.7441 - val_loss: 0.8219 - val_binary_crossentropy: 0.8219 - val_acc: 0.5259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.82187, saving model to model/CNN_sub_10.h1\n",
      "Epoch 2/3\n",
      "714729/714729 [==============================] - 502s 703us/step - loss: 0.5083 - binary_crossentropy: 0.5083 - acc: 0.7642 - val_loss: 0.7133 - val_binary_crossentropy: 0.7133 - val_acc: 0.6051\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.82187 to 0.71331, saving model to model/CNN_sub_10.h1\n",
      "Epoch 3/3\n",
      "714729/714729 [==============================] - 502s 703us/step - loss: 0.5021 - binary_crossentropy: 0.5021 - acc: 0.7690 - val_loss: 0.6800 - val_binary_crossentropy: 0.6800 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71331 to 0.67998, saving model to model/CNN_sub_10.h1\n"
     ]
    }
   ],
   "source": [
    "for ii in range(1,len(feat_all)-1):\n",
    "    # Select features and load into X and Y\n",
    "    features=['jetImages']+[feat_all[ii]]\n",
    "    label=['labels']\n",
    "    X_train,Y_train=build_XY(features,label,data_train)\n",
    "    # Build Model\n",
    "    circleCNN = build_model()\n",
    "    # Fit \n",
    "    checkpoint_path = \"model/CNN_sub_\"+str(ii)+\".h1\"\n",
    "    if not os.path.exists(\"model\"):\n",
    "        os.mkdir(\"model\")\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', \n",
    "                                   verbose=1, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', \n",
    "                                   period=1)    \n",
    "    EPOCHS = 3\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "    history = circleCNN.fit(\n",
    "      X_train, Y_train,\n",
    "      epochs=EPOCHS, validation_split = 0.2, verbose = 1,\n",
    "      callbacks=[early_stop, model_checkpoint])\n",
    "    circleCNN.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Absolute Jet_pull seperately\n",
    "This was added after other Variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/20\n",
      "714729/714729 [==============================] - 586s 820us/step - loss: 0.5261 - binary_crossentropy: 0.5261 - acc: 0.7493 - val_loss: 0.7375 - val_binary_crossentropy: 0.7375 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73754, saving model to model/CNN_sub_11.h1\n",
      "Epoch 2/20\n",
      "714729/714729 [==============================] - 599s 838us/step - loss: 0.5016 - binary_crossentropy: 0.5016 - acc: 0.7685 - val_loss: 0.7052 - val_binary_crossentropy: 0.7052 - val_acc: 0.6052\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73754 to 0.70521, saving model to model/CNN_sub_11.h1\n",
      "Epoch 3/20\n",
      "564864/714729 [======================>.......] - ETA: 2:07 - loss: 0.4994 - binary_crossentropy: 0.4994 - acc: 0.7697"
     ]
    }
   ],
   "source": [
    "# Select features and load into X and Y\n",
    "features=['jetImages']+[feat_all[11]]\n",
    "label=['labels']\n",
    "X_train,Y_train=build_XY(features,label,dic)\n",
    "# Build Model\n",
    "circleCNN = build_model()\n",
    "# Fit \n",
    "checkpoint_path = \"model/CNN_sub_\"+str(11)+\".h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True, \n",
    "                               save_weights_only=False, mode='auto', \n",
    "                               period=1)    \n",
    "EPOCHS = 20\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "history = circleCNN.fit(\n",
    "  X_train, Y_train,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose = 1,\n",
    "  callbacks=[early_stop, model_checkpoint])\n",
    "circleCNN.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
