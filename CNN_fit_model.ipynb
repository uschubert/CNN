{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras.utils\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN():\n",
    "    input2 = [layers.Input(shape = (len(X_train[i][0]),)) for i in range(1,len(X_train))]\n",
    "    input1 = layers.Input(shape = (grid, grid,1))\n",
    "    x = layers.Conv2D(32, (5, 5), activation = 'relu', padding = 'same')(input1)\n",
    "    x = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x1 = layers.Flatten()(x)\n",
    "    if input2 == []:\n",
    "        x = layers.Dense(64, activation='relu')(x1)\n",
    "    else:\n",
    "        x = layers.concatenate(inputs = [x1] + input2, axis = -1)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "    output = layers.Dense(2, activation='softmax')(x)\n",
    "    model = models.Model(inputs= [input1] + input2,\n",
    "                         outputs = output)\n",
    "    opt = keras.optimizers.Adam(lr = 0.0005,\n",
    "                                beta_1 = 0.9,\n",
    "                                beta_2 = 0.9,\n",
    "                                amsgrad = False)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DNN():\n",
    "    input2 = [layers.Input(shape = (len(X_train[i][0]),)) for i in range(0,len(X_train))]\n",
    "    if len(input2) == 1:\n",
    "        x = layers.Dense(64,activation='relu')(input2[0])\n",
    "    else:\n",
    "        x = layers.concatenate(inputs = input2, axis = -1)\n",
    "        x = layers.Dense(64, activation = 'relu')(x)\n",
    "    output = layers.Dense(2, activation = 'softmax')(x)\n",
    "    model = models.Model(inputs = input2, \n",
    "                         outputs = output)\n",
    "    opt=keras.optimizers.Adam(lr = 0.0005,\n",
    "                              beta_1 = 0.9,\n",
    "                              beta_2 = 0.9,\n",
    "                              amsgrad = False)\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = opt,\n",
    "                metrics = ['binary_crossentropy', 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_XY(features,label,dic):\n",
    "    X = [dic[key] for key in features]\n",
    "    Y = [dic[key] for key in label]\n",
    "    dim = [ele.shape+(1,) for ele in X]\n",
    "    for i in range(0,len(features)):\n",
    "        X[i] = X[i].reshape(dim[i])\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data:\n",
    "Using Train and Test set produced by preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 16\n",
    "data_train = np.load('../data/ShowJet_full_train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696322\n"
     ]
    }
   ],
   "source": [
    "n_train = len(data_train['jetPt'])\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetImages',\n",
       " 'jetPt',\n",
       " 'jetEta',\n",
       " 'jetPhi',\n",
       " 'tau21',\n",
       " 'chMult',\n",
       " 'neutMult',\n",
       " 'phoMult',\n",
       " 'eleMult',\n",
       " 'muMult',\n",
       " 'jetpull']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all = [key for key in data_train.keys()];feat_all.remove('labels');feat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with only Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features = ['jetImages']\n",
    "\n",
    "# label into Y\n",
    "label = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_XY(features,label,data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "CNN = build_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 9ms/step\n",
      "[[0.50002486 0.4999752 ]\n",
      " [0.50006497 0.49993497]\n",
      " [0.5000201  0.49997988]\n",
      " [0.5000378  0.49996218]\n",
      " [0.5000567  0.49994338]\n",
      " [0.5000075  0.49999255]\n",
      " [0.5000034  0.4999966 ]\n",
      " [0.50011414 0.4998858 ]\n",
      " [0.49999133 0.50000864]\n",
      " [0.5001163  0.4998837 ]]\n",
      "[0.6932005882263184, 0.6932005882263184, 0.5]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/schubert/.conda/envs/LRP/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 950s 700us/step - loss: 0.5700 - binary_crossentropy: 0.5700 - acc: 0.7139 - val_loss: 0.8721 - val_binary_crossentropy: 0.8721 - val_acc: 0.4338\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87208, saving model to model/CNN_sub_0.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 941s 693us/step - loss: 0.5450 - binary_crossentropy: 0.5450 - acc: 0.7355 - val_loss: 0.7963 - val_binary_crossentropy: 0.7963 - val_acc: 0.4724\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87208 to 0.79631, saving model to model/CNN_sub_0.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 942s 694us/step - loss: 0.5412 - binary_crossentropy: 0.5412 - acc: 0.7405 - val_loss: 0.8012 - val_binary_crossentropy: 0.8012 - val_acc: 0.5281\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.79631\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_sub_0.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 3\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS, \n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with all Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all possible features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features will be loaded into X\n",
    "features = feat_all\n",
    "\n",
    "# label into Y\n",
    "label = ['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_XY(features,label,data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = build_CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's prediction $before$ training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step\n",
      "[[0.48357704 0.5164229 ]\n",
      " [0.48567775 0.5143223 ]\n",
      " [0.48854178 0.5114582 ]\n",
      " [0.48936164 0.5106383 ]\n",
      " [0.4840711  0.5159289 ]\n",
      " [0.49210805 0.50789195]\n",
      " [0.4878     0.51219994]\n",
      " [0.48915666 0.51084334]\n",
      " [0.48394787 0.5160522 ]\n",
      " [0.48813173 0.5118683 ]]\n",
      "[0.697828471660614, 0.697828471660614, 0.4000000059604645]\n"
     ]
    }
   ],
   "source": [
    "X_batch = [ele[:10] for ele in X_train]\n",
    "Y_batch = [ele[:10] for ele in Y_train]\n",
    "example_result = CNN.predict(x = X_batch)\n",
    "results = CNN.evaluate(x = X_batch, y = Y_batch )\n",
    "print(example_result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train! (warning: if building CNN, computer tends to get loud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/4\n",
      "1357057/1357057 [==============================] - 974s 718us/step - loss: 0.4427 - binary_crossentropy: 0.4427 - acc: 0.7981 - val_loss: 0.6956 - val_binary_crossentropy: 0.6956 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69556, saving model to model/CNN_all.h1\n",
      "Epoch 2/4\n",
      "1357057/1357057 [==============================] - 987s 728us/step - loss: 0.4231 - binary_crossentropy: 0.4231 - acc: 0.8104 - val_loss: 0.5873 - val_binary_crossentropy: 0.5873 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69556 to 0.58728, saving model to model/CNN_all.h1\n",
      "Epoch 3/4\n",
      "1357057/1357057 [==============================] - 1043s 769us/step - loss: 0.4184 - binary_crossentropy: 0.4184 - acc: 0.8126 - val_loss: 0.6007 - val_binary_crossentropy: 0.6007 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58728\n",
      "Epoch 4/4\n",
      "1357057/1357057 [==============================] - 1283s 945us/step - loss: 0.4165 - binary_crossentropy: 0.4165 - acc: 0.8138 - val_loss: 0.5565 - val_binary_crossentropy: 0.5565 - val_acc: 0.7229\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58728 to 0.55647, saving model to model/CNN_all.h1\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model/CNN_all.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                   verbose = 1, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', \n",
    "                                                   period = 1)    \n",
    "EPOCHS = 4\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "history = CNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build 10 Models with one expert Variable each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1088s 801us/step - loss: 0.5608 - binary_crossentropy: 0.5608 - acc: 0.7201 - val_loss: 0.8409 - val_binary_crossentropy: 0.8409 - val_acc: 0.4888\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84089, saving model to model/CNN_sub_1.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1237s 912us/step - loss: 0.5344 - binary_crossentropy: 0.5344 - acc: 0.7416 - val_loss: 0.7971 - val_binary_crossentropy: 0.7971 - val_acc: 0.5348\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84089 to 0.79709, saving model to model/CNN_sub_1.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1223s 902us/step - loss: 0.5295 - binary_crossentropy: 0.5295 - acc: 0.7461 - val_loss: 0.7386 - val_binary_crossentropy: 0.7386 - val_acc: 0.5640\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.79709 to 0.73860, saving model to model/CNN_sub_1.h1\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1151s 848us/step - loss: 0.5708 - binary_crossentropy: 0.5708 - acc: 0.7130 - val_loss: 0.9080 - val_binary_crossentropy: 0.9080 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90800, saving model to model/CNN_sub_2.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1089s 803us/step - loss: 0.5472 - binary_crossentropy: 0.5472 - acc: 0.7330 - val_loss: 0.9420 - val_binary_crossentropy: 0.9420 - val_acc: 0.4331\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.90800\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1135s 837us/step - loss: 0.5412 - binary_crossentropy: 0.5412 - acc: 0.7392 - val_loss: 0.8144 - val_binary_crossentropy: 0.8144 - val_acc: 0.4770\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90800 to 0.81443, saving model to model/CNN_sub_2.h1\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1069s 788us/step - loss: 0.5762 - binary_crossentropy: 0.5762 - acc: 0.7083 - val_loss: 0.9282 - val_binary_crossentropy: 0.9282 - val_acc: 0.3682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92824, saving model to model/CNN_sub_3.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1037s 764us/step - loss: 0.5493 - binary_crossentropy: 0.5493 - acc: 0.7306 - val_loss: 0.8800 - val_binary_crossentropy: 0.8800 - val_acc: 0.4698\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.92824 to 0.87995, saving model to model/CNN_sub_3.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1041s 767us/step - loss: 0.5395 - binary_crossentropy: 0.5395 - acc: 0.7397 - val_loss: 0.7955 - val_binary_crossentropy: 0.7955 - val_acc: 0.5321\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.87995 to 0.79550, saving model to model/CNN_sub_3.h1\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1051s 774us/step - loss: 0.5178 - binary_crossentropy: 0.5178 - acc: 0.7484 - val_loss: 0.8467 - val_binary_crossentropy: 0.8467 - val_acc: 0.5414\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84671, saving model to model/CNN_sub_4.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1047s 772us/step - loss: 0.5138 - binary_crossentropy: 0.5138 - acc: 0.7518 - val_loss: 0.7571 - val_binary_crossentropy: 0.7571 - val_acc: 0.6013\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84671 to 0.75707, saving model to model/CNN_sub_4.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1043s 769us/step - loss: 0.5142 - binary_crossentropy: 0.5142 - acc: 0.7518 - val_loss: 0.7199 - val_binary_crossentropy: 0.7199 - val_acc: 0.6104\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75707 to 0.71987, saving model to model/CNN_sub_4.h1\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1061s 782us/step - loss: 0.5384 - binary_crossentropy: 0.5384 - acc: 0.7341 - val_loss: 0.7828 - val_binary_crossentropy: 0.7828 - val_acc: 0.5098\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78282, saving model to model/CNN_sub_5.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1059s 780us/step - loss: 0.5111 - binary_crossentropy: 0.5111 - acc: 0.7586 - val_loss: 0.7149 - val_binary_crossentropy: 0.7149 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78282 to 0.71493, saving model to model/CNN_sub_5.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1057s 779us/step - loss: 0.5042 - binary_crossentropy: 0.5042 - acc: 0.7648 - val_loss: 0.6842 - val_binary_crossentropy: 0.6842 - val_acc: 0.5913\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71493 to 0.68423, saving model to model/CNN_sub_5.h1\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1065s 785us/step - loss: 0.5560 - binary_crossentropy: 0.5560 - acc: 0.7237 - val_loss: 0.7655 - val_binary_crossentropy: 0.7655 - val_acc: 0.5065\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76554, saving model to model/CNN_sub_6.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1058s 779us/step - loss: 0.5320 - binary_crossentropy: 0.5320 - acc: 0.7455 - val_loss: 0.7368 - val_binary_crossentropy: 0.7368 - val_acc: 0.5598\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76554 to 0.73677, saving model to model/CNN_sub_6.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1056s 778us/step - loss: 0.5288 - binary_crossentropy: 0.5288 - acc: 0.7490 - val_loss: 0.8134 - val_binary_crossentropy: 0.8134 - val_acc: 0.5253\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.73677\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1067s 786us/step - loss: 0.5456 - binary_crossentropy: 0.5456 - acc: 0.7295 - val_loss: 0.8347 - val_binary_crossentropy: 0.8347 - val_acc: 0.5004\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.83466, saving model to model/CNN_sub_7.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1061s 782us/step - loss: 0.5146 - binary_crossentropy: 0.5146 - acc: 0.7569 - val_loss: 0.8014 - val_binary_crossentropy: 0.8014 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.83466 to 0.80137, saving model to model/CNN_sub_7.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1028s 758us/step - loss: 0.5136 - binary_crossentropy: 0.5136 - acc: 0.7596 - val_loss: 0.8325 - val_binary_crossentropy: 0.8325 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.80137\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1035s 763us/step - loss: 0.5727 - binary_crossentropy: 0.5727 - acc: 0.7111 - val_loss: 0.6472 - val_binary_crossentropy: 0.6472 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64717, saving model to model/CNN_sub_8.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1036s 763us/step - loss: 0.5554 - binary_crossentropy: 0.5554 - acc: 0.7257 - val_loss: 0.7889 - val_binary_crossentropy: 0.7889 - val_acc: 0.5106\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.64717\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1034s 762us/step - loss: 0.5426 - binary_crossentropy: 0.5426 - acc: 0.7378 - val_loss: 0.7022 - val_binary_crossentropy: 0.7022 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.64717\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 1049s 773us/step - loss: 0.5668 - binary_crossentropy: 0.5668 - acc: 0.7160 - val_loss: 0.8450 - val_binary_crossentropy: 0.8450 - val_acc: 0.4868\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84498, saving model to model/CNN_sub_9.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 1040s 766us/step - loss: 0.5443 - binary_crossentropy: 0.5443 - acc: 0.7353 - val_loss: 0.8275 - val_binary_crossentropy: 0.8275 - val_acc: 0.4839\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84498 to 0.82751, saving model to model/CNN_sub_9.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 1035s 763us/step - loss: 0.5398 - binary_crossentropy: 0.5398 - acc: 0.7395 - val_loss: 0.8206 - val_binary_crossentropy: 0.8206 - val_acc: 0.5199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss improved from 0.82751 to 0.82061, saving model to model/CNN_sub_9.h1\n",
      "Train on 1357057 samples, validate on 339265 samples\n",
      "Epoch 1/3\n",
      "1357057/1357057 [==============================] - 967s 713us/step - loss: 0.5739 - binary_crossentropy: 0.5739 - acc: 0.7103 - val_loss: 0.7832 - val_binary_crossentropy: 0.7832 - val_acc: 0.4769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78316, saving model to model/CNN_sub_10.h1\n",
      "Epoch 2/3\n",
      "1357057/1357057 [==============================] - 964s 711us/step - loss: 0.5492 - binary_crossentropy: 0.5492 - acc: 0.7318 - val_loss: 0.7811 - val_binary_crossentropy: 0.7811 - val_acc: 0.5347\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78316 to 0.78114, saving model to model/CNN_sub_10.h1\n",
      "Epoch 3/3\n",
      "1357057/1357057 [==============================] - 966s 712us/step - loss: 0.5429 - binary_crossentropy: 0.5429 - acc: 0.7386 - val_loss: 0.8319 - val_binary_crossentropy: 0.8319 - val_acc: 0.4943\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.78114\n"
     ]
    }
   ],
   "source": [
    "for ii in range(1,len(feat_all)):\n",
    "    # Select features and load into X and Y\n",
    "    features = ['jetImages'] + [feat_all[ii]]\n",
    "    label = ['labels']\n",
    "    X_train,Y_train = build_XY(features,label,data_train)\n",
    "    \n",
    "    # Build Model\n",
    "    circleCNN = build_CNN()\n",
    "    \n",
    "    # Fit \n",
    "    checkpoint_path = \"model/CNN_sub_\"+str(ii)+\".h1\"\n",
    "    if not os.path.exists(\"model\"):\n",
    "        os.mkdir(\"model\")\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                                                       verbose = 1, save_best_only = True, \n",
    "                                                       save_weights_only = False, mode = 'auto', \n",
    "                                                       period = 1)    \n",
    "    EPOCHS = 3\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "    history = circleCNN.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs = EPOCHS,\n",
    "        validation_split = 0.2,\n",
    "        verbose = 1,\n",
    "        callbacks = [early_stop, model_checkpoint])\n",
    "    circleCNN.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN\n",
    "where we only use expert variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with all Expert Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 714729 samples, validate on 178683 samples\n",
      "Epoch 1/4\n",
      "714729/714729 [==============================] - 29s 41us/step - loss: 0.4268 - binary_crossentropy: 0.4268 - acc: 0.8137 - val_loss: 0.5848 - val_binary_crossentropy: 0.5848 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58480, saving model to model/DNN_all.h1\n",
      "Epoch 2/4\n",
      "714729/714729 [==============================] - 29s 41us/step - loss: 0.4085 - binary_crossentropy: 0.4085 - acc: 0.8254 - val_loss: 0.6051 - val_binary_crossentropy: 0.6051 - val_acc: 0.7192\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58480\n",
      "Epoch 3/4\n",
      "714729/714729 [==============================] - 30s 42us/step - loss: 0.4052 - binary_crossentropy: 0.4052 - acc: 0.8269 - val_loss: 0.5776 - val_binary_crossentropy: 0.5776 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58480 to 0.57756, saving model to model/DNN_all.h1\n",
      "Epoch 4/4\n",
      "714729/714729 [==============================] - 27s 38us/step - loss: 0.4034 - binary_crossentropy: 0.4034 - acc: 0.8278 - val_loss: 0.5611 - val_binary_crossentropy: 0.5611 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.57756 to 0.56112, saving model to model/DNN_all.h1\n"
     ]
    }
   ],
   "source": [
    "features = feat_all[1:]\n",
    "label = ['labels']\n",
    "X_train,Y_train = build_XY(features,label,data_train)\n",
    "# Build Model\n",
    "circleCNN = build_DNN()\n",
    "# Fit \n",
    "checkpoint_path = \"model/DNN_all.h1\"\n",
    "if not os.path.exists(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
    "                               verbose = 1, save_best_only = True, \n",
    "                               save_weights_only = False, mode = 'auto', \n",
    "                               period = 1)    \n",
    "EPOCHS = 4\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 30)\n",
    "history = circleCNN.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stop, model_checkpoint])\n",
    "circleCNN.save(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
